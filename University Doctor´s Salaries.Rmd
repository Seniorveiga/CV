---
title: "Analysis of a wrong lineal model in university teacher´s salaries."
output: html_notebook
---

# Enunciado

The dataset *"Salaries.csv "* is taken. The dataset consists of nine months' salaries collected from 397 university professors in the United States during 2008 and 2009. In addition to salaries, the professor's rank, gender, discipline, years since Ph.D., and years of service were also collected.

The objective of this internship is to perform a full study of the dataset in order to implement a regularized linear model that makes predictions about a professor's salary to be received. Also, it will be asked to take advantage of the explainability of these models and the statistical studies performed to yield intuitions and dependencies in the data.

Information about the columns:

1.  **rank**: Categorical - from assistant professor, associate professor or full professor
2.  **discipline**: Categorical - Type of department in which the professor works, either applied (B) or theoretical (A)
3.  **yrs.since.phd**: Continuous - Number of years since the professor earned his/her Ph.D.
4.  **yrs.service**: Continuous - Number of years the professor has served the department and/or the university.
5.  **sex**: Categorical - Sex of the professor, male or female
6.  **salary**: Continuous - Professor's nine-month salary (USD)

# Section

## Section 1

1 - Load data. Perform a visual inspection of the distribution of salaries by variables for each attribute. Which variables are best to separate the data?

First, we load the data into a variable that identifies the DataFrame:

```{r}
df_prof <- read.csv("Salaries.csv")
head(df_prof)
str(df_prof)
```

It asks us to perform a salary distribution depending on each attribute. This is equivalent to make a Facet grid for each of the categories.

Let's see if all the variables are ready to be plotted, which should be converted to *"factor "* type:

```{r}
library(dplyr)   #Importamos dplyr para usar la función "count" de este paquete.
summary(df_prof)  

count(df_prof,salary, sort = TRUE)
count(df_prof,yrs.since.phd, sort = TRUE)

```

As we can see, some of them are of type *"char "* then they would have to be transformed to be able to plot them.

```{r}
df_prof$rank <- as.factor(df_prof$rank)
```

### Graphics

We import *"ggplot2"* and then make the various plots.

We use new arguments based on the following *ggplot* documentation: <https://www.rdocumentation.org/packages/ggplot2/versions/0.9.1/topics/geom_histogram>

```{r}
salaries_rank <-ggplot(data = df_prof, aes(x = salary, fill = rank))
salaries_rank + geom_histogram(binwidth = 10000, color = "black") + ggtitle("Professor´s salary according to rank.")

salaries_discipline <-ggplot(data = df_prof, aes(x = salary, fill = discipline))
salaries_discipline + geom_histogram(binwidth = 10000, color = "black") + ggtitle("Professor´s salary according to discipline.")

salaries_phd <-ggplot(data = df_prof, aes(x = salary, y = yrs.since.phd))
salaries_phd + geom_point() + ggtitle("Professor´s salary according to the years as a doctor.")

salaries_service <-ggplot(data = df_prof, aes(x = salary, y = yrs.service))
salaries_service + geom_point() + ggtitle("Professor´s salary according to the years of experience.")

salaries_sex <-ggplot(data = df_prof, aes(x = salary, fill = sex))
salaries_sex + geom_histogram(binwidth = 10000, color = "black")  + ggtitle("Professor´s salary according to sex.")

```

### Hypothesis tests

We can see by eye that there are variables that stand out more than others:

-   We can see that there are variables that are going to have virtually no weight such as **sex**. Although at first glance it may seem that men earn more than women, to reach solid statistical conclusions about gender we would need a larger sample of women because we have very few elements of this gender.

```{r}
count(df_prof, sex)
```

We will see more about this variable in exercise 2.

-   Regarding the *discipline* variable, I doubt that it will help us to create differences in salaries. If we look at it, it seems that in both cases they are following a similar distribution or at least their means are very similar.

In line with what we have seen in this topic, we calculate their means and we can do a hypothesis test to see if they match. We can do two tests that are appropriate for this situation:

-   Student's $\mathit{t}$ test: it tells us whether the means of both populations are significantly the same.

-   Kolmogorov-Smirnov test: It allows us to see if both samples follow the same distribution.

**Very important**: Since we are dealing with a sample that does not have a random component, we can NOT apply the ***Central Limit Theorem*** and ensure that it approximates a normal distribution! (Note that the samples in *discipline = A* and *discipline = B* are greater than 30 which is the usual condition but no others are satisfied). However, we will check this through the Shapiro-Wilk test.

In addition, as it is a matter of salary, there are usually ranks and categories, some higher than others, so we can guess that it is not normal.

Discipline A:

-   $H_{0}$: Discipline A follows a normal distribution.
-   $H_{1}$: Discipline A does not follow a normal distribution.

Significance level $\alpha$=0.05.

```{r}
shapiro.test(df_prof$salary[df_prof$discipline == "A"])
```

If we do it in discipline B:

-   $H_{0}$: Discipline B follows a normal distribution.
-   $H_{1}$: Discipline B does not follow a normal distribution.

Significance level $\alpha$=0.05.

```{r}
shapiro.test(df_prof$salary[df_prof$discipline == "B"])
```

We see that in both cases the $\mathit{p}$-value is less than 0.05 so **is not considered to follow a normal distribution**. This also rules out the use of the Student's $\mathit{t}$-test because we do not have the conditions to apply it (The data do not follow a normal distribution).

At the very least, we can calculate the mean and standard deviation to see how much the two groups differ:

```{r}
mean(df_prof$salary[df_prof$discipline=="A"])
mean(df_prof$salary[df_prof$discipline=="B"])

sqrt(var(df_prof$salary[df_prof$discipline=="A"]))
sqrt(var(df_prof$salary[df_prof$discipline=="B"]))
```

We can see that the differences between one and the other are not particularly large, especially considering the standard deviation. It could have some impact due to the difference in means, but that is all. Surely the other variables have more weight.

```{r}
salaries_phd <-ggplot(data = df_prof, aes(x = salary, y = yrs.since.phd))
salaries_phd + geom_point() + ggtitle("Professor´s salary according to the years as a doctor.") + geom_smooth(method = "lm", se = FALSE)

salaries_service <-ggplot(data = df_prof, aes(x = salary, y = yrs.service))
salaries_service + geom_point() + ggtitle("Professor´s salary according to the years of experience.") + geom_smooth(method = "lm", se = FALSE)

cor_phd <- cor(df_prof$salary, df_prof$yrs.since.phd)
cor_yrs <- cor(df_prof$salary, df_prof$yrs.service)

cor_phd 
cor_yrs
```

We can see that there is a moderate correlation in both variables, both in the time as a doctor and the years of experience behind, with the years as a doctor having the greatest impact, although by a slight difference.

Finally, with respect to the rank of the professor, we can perform another type of test, the **Kruskal-Wallis**, as it allows us to compare the categories one by one to see if there are statistically significant differences.

**Very important**: Note that coming from a distribution that does not follow a normal distribution, we cannot perform the **Anova** test or its **Pairwise** version.

Previsualizing the values:

```{r}

#salaries_rank <-ggplot(data = df_prof, aes(x = salary, fill = rank))
#salaries_rank + geom_histogram(binwidth = 10000, color = "black") + ggtitle("Professor´s salary according to rank.")

kruskal.test(salary ~ rank, data = df_prof)
```

Since the $\mathit{p}$-value is less than the significance level $\alpha$ = 0.05, then we arrive at rejecting $H_{0}$ and therefore accept $H_{1}$ which tells us that there are statistically significant differences between the three groups.

To see what those groups are, we do a *Wilcoxon*:

```{r}
help(pairwise.wilcox.test) #Usualmente empleo Bonferroni así que uso ese ajuste
```

```{r}
pairwise.wilcox.test(df_prof$salary, df_prof$rank,
                 p.adjust.method = "bonferroni")
```

The three groups are significantly VERY different from each other, and given their level of significance, it is clear that they influence the teacher's final salary.

Therefore, the variables that I consider influential, from highest to lowest are:

-   Rank (highly marked p-value)

-   yrs.since.phd

-   yrs.service

While those with little or no influence are:

-   discipline (Means within the standard error and similar standard errors).

-   sex (not enough samples)

## Section 2

2 - Is it possible to use a parametric test to determine whether the mean wages between men and women are the same or different? It should be borne in mind that, as long as a parametric test is requested, it must be determined whether the samples meet the necessary hypotheses.

For a parametric test to be performed, it must be fulfilled that the data we are working with must follow a normal distribution. This can be checked by performing a Shapiro-Wilk test on each of these subgroups, as we have done with the discipline variable.

Let's see if they follow a normal distribution. Our hypothesis test has the following two hypotheses:

-   $H_{0}$: Men's wages follow a normal distribution.
-   $H_{1}$: Men's wages do not follow a normal distribution.

With a significance level $\alpha$ = 0.01

```{r}
shapiro.test(df_prof$salary[df_prof$sex == "Male"])
```

Since we have that the $\mathit{p}$-value is less than $\alpha$, then we discard the null hypothesis and conclude that they do not follow a normal distribution with 99% significance.

In the case of the female group:

-   $H_{0}$: Women's wages follow a normal distribution.
-   $H_{1}$: Women's wages do not follow a normal distribution.

With a significance level of $H_{1}$\$ = 0.01

```{r}
shapiro.test(df_prof$salary[df_prof$sex == "Female"])
```

We could say that they follow a normal distribution, but we reject the test by a very small margin.

Although the girls' group does pass the test, we cannot compare the two groups because the normality condition for comparing means (Student's $\mathit{t}$ test) is not met.

However, we can use the **Wilcoxon-Man-Whitney test** as they are non-parametric, and it is the alternative to Student's $\mathit{t}$ test.

Hypothesis:

-   $H_{0}$: Women's and men's wages are statistically equal.
-   $H_{1}$: Women's and men's wages are significantly different.

With a significance level of $\alpha$ = 0.05.

```{r}
wilcox.test(df_prof$salary[df_prof$sex == "Female"], df_prof$salary[df_prof$sex == "Male"])
```

Since the $\mathit{p}$-value is small enough, they are considered to be significantly different, but the test, again, does not have a very robust security as it is close to the $\mathit{p}$-value.

```{r}
mean(df_prof$salary[df_prof$sex == "Female"])
mean(df_prof$salary[df_prof$sex == "Male"])
```

We conclude that they are different, slightly higher in men but ideally we would like to have more samples of teachers who are women, as there are only 39.

## Section 3

3 - Split the dataset taking the first 317 instances as train and the last 80 as test. Train a linear regression model with Ridge and Lasso regularization on train selecting the one with the best MSE. Give the metrics in test. To evaluate the use of the one hot encoder, and, if used, to argue it.

First of all, we see if the One-Hot Encoder should be used. Using it is important because in prediction models numbers are used and we can generate Boolean variables that indicate different categories. In this case **One Hot Encoder** should be implemented since we have three variables that can give us problems:

-   The "sex" column.
-   The "rank" column.
-   The column "service".

```{r}
df_profOHE <- model.matrix(salary~.-1,df_prof)
head(df_profOHE)
```

We divide the dataset into two sets. What we have to predict, again, is the salary so we have to isolate the variable *"salary "* and put it in two subsets of 317 and 80 elements.

**Important**: In the data set X we take from column 2 because the first one, which is included in the DataFrame is a column with the positions of the rows, and therefore does not give us any relevant information.

```{r}
df_prof_X <- data.matrix(df_profOHE[,2:8]) #Tomamos columnas 2 a 6, evitando "index"
# df_prof_X

df_prof_y <- data.matrix(df_prof$salary) #Variable objetivo
# df_prof_y
```

**Note that we have taken the salaries of the DataFrame that does not contain the One Hot Encoder**.

```{r}
df_prof_X_train <- df_prof_X[1:318,]  #No cuenta el último elemento
df_prof_X_test <- df_prof_X[318:397,]

df_prof_y_train <- df_prof_y[1:318,]
df_prof_y_test <- df_prof_y[318:397,]
```

We are asked to train a GLM, which can use either Ridge or Lasso. Since this is a linear, non-logistic regression, we use the MSE measure and load the *glmnet* library. If we want to apply a Ridge model, being a higher case of a GLM, we should specify an \$alpha of 0.

**Important** when we do a regularization, it is a type of **cross-validation** so we have to prefix it with cv. Also important to note that the **cv.glmnet** function only accepts arrays as possible input values.

### Ridge regularization

```{r}
library(glmnet)
set.seed(1)

# Regularización Ridge
cv.prof_ridge <- cv.glmnet(df_prof_X_train, df_prof_y_train, family='gaussian', alpha=0, type.measure='mse')

# Gráfica
plot(cv.prof_ridge)

# Mínimo para ajustar valor
cv.prof_ridge$lambda.min

# Error esperado para este lambda mínimo
min(cv.prof_ridge$cvm)
```

We see that the error is very large.

If we look at its coefficients:

```{r}
coef(cv.prof_ridge, s=cv.prof_ridge$lambda.min)
```

And we try to make a prediction:

```{r}
print("Valores predichos por la regresion lineal con Regularización Ridge: 5 primeros valores")
predict.glmnet(cv.prof_ridge$glmnet.fit, newx=df_prof_X_test[1:5,], s=cv.prof_ridge$lambda.min)
print("Valores reales")
df_prof_y_test[1:5]    #Nótese que ya no es una matriz sino un array
```

### Lasso regularization

The procedure is the same but as we work with GLM we have to change the parameter to $\alpha$ = 1.

```{r}
set.seed(1)
# Regularización Lasso
cv.prof_lasso <- cv.glmnet(df_prof_X_train, df_prof_y_train, family='gaussian', alpha=1, type.measure='mse')

# Gráfica
plot(cv.prof_lasso)

# Mínimo para ajustar valor
cv.prof_lasso$lambda.min

# Error esperado para este lambda mínimo
min(cv.prof_lasso$cvm)
```

Again, it comes out again that it has a very large error, 585488782.

If we look at its coefficients:

```{r}
coef(cv.prof_lasso, s=cv.prof_lasso$lambda.min)
```

Y, de nuevo, tratamos de hacer una predicción:

```{r}
print("Valores predichos por la regresion lineal con Regularización Lasso:  5 primeros valores")
predict.glmnet(cv.prof_ridge$glmnet.fit, newx=df_prof_X_test[1:5,], s=cv.prof_ridge$lambda.min)
print("Valores reales")
df_prof_y_test[1:5]
```

### Which model should we choose?

Knowing that we should choose the model with the lowest MSE, we look at its values. The MSE is obtained by choosing from the cross validation the errors, and from them the one that has the minimum associated MSE.

```{r}
mse_ridge <- cv.prof_ridge$cvm[cv.prof_ridge$lambda == cv.prof_ridge$lambda.min]
mse_lasso <- cv.prof_lasso$cvm[cv.prof_lasso$lambda == cv.prof_lasso$lambda.min]

# Los valores de antes.
mse_ridge
mse_lasso
```

In both cases it is very large, but we should choose the Ridge one since it is a little smaller. Given how similar they are, it is even possible that it depends on the seed (By mistake, I have done it before with the whole dataset instead of the train and I got the Lasso's one with less error, so it is indifferent one than the other).

### Metrics

```{r}
summary(cv.lasso)
```

Como vemos, el summary() no nos da esta información por lo que hay que conseguirla de otra forma.

```{r}
#MSE
mse_ridge
mse_lasso

#R^2
r2_ridge = 1 - mse_ridge/var(df_prof_y)
r2_lasso = 1 - mse_lasso/var(df_prof_y)
r2_ridge
r2_lasso
```

Due to:

$$R^{2} = 1 - \frac{\sigma^{2}_{r}}{\sigma^{2}_{y}} = 1 - \frac{MSE}{\sigma^{2}_{y}}$$

Note that in either case the predictive power is low, and we are also taking into account the cases that are already trained. If we do it with those that are totally new, i.e. the *"test "* cases:

```{r}
#R^2
y_pred_ridge <- predict.glmnet(cv.prof_ridge$glmnet.fit, newx = df_prof_X_test, s = cv.prof_ridge$lambda.min) 
r2_ridge <- cor(y_pred_ridge, df_prof_y_test)^2

y_pred_lasso <- predict.glmnet(cv.prof_lasso$glmnet.fit, newx = df_prof_X_test, s = cv.prof_lasso$lambda.min) 
r2_lasso <- cor(y_pred_lasso, df_prof_y_test)^2

r2_ridge
r2_lasso
```

If we take a closer look, the values are much lower.

It is worth noting that the variable **"AssocProf"**, i.e. associate professor, has been completely dropped from the model.

It is also curious to see that, if we look at the correlations, gender seems to have a positive influence in the case of being male and is more important than the rest. However, it should be noted that given their low R^2 the confidence that these coefficients provide us with is doubtful, especially those coming from samples where no clear conclusions could be drawn.

## Section 4

4 - Study the normality of the residuals of the resulting model. Is any bias detected?

We are asked to see that the residuals are normal. In our case we will take Ridge's because there is a slight difference in his favor. Here or we can use residuals, but since we know that it is the difference between the actual value and the value of the estimates, we can calculate it by subtracting both vectors:

```{r}
# df_prof_y_test    Valores reales
# y_pred_ridge      Predicciones
residuals_ridge <- df_prof_y_test - y_pred_ridge

residuals_ridge
```

To see if they follow a normal distribution, we can directly apply a hypothesis test, specifically we will have to **perform a Shapiro-Wilk test**.

In addition, we are going to represent the data beforehand in a histogram to get an idea of how they may be distributed, bearing in mind that the closer they are to zero, the more reliable they are with respect to our data!

```{r}
df_res_ridge <- data.frame(residuals_ridge)
```

```{r}
visualization_res_ridge <-ggplot(data = df_res_ridge, aes(x=s1))
visualization_res_ridge + geom_histogram(aes(y = after_stat(density)),binwidth = 10000, fill = "pink",color = "black") +
geom_density(color="red",size=1) + 
ggtitle("Professor´s salary according to rank.")
```

We may have some doubts because, as we can see most values are centered on zero, but it has a certain shape.

Let's check it with a hypothesis test:

Null and alternative hypothesis:

-   $H_{0}$: The residuals of the linear model with Ridge regularization follow a normal distribution.
-   $H_{1}$: The residuals of the linear model with Ridge regularization do not follow a normal distribution.

Significance level $\alpha$=0.05.

```{r}
shapiro.test(residuals_ridge)
```
  
Since the $\mathit{p}$-value is less than $\alpha$, then we reject $H_{0}$ and conclude that it does not follow a normal distribution.

Furthermore, we indeed see that there is a bias because it approaches the maximum from the left and from the right in the same way, when we see that there is a tendency to much larger positive residuals.

This translates into the fact that our line tends to give lower results than the actual results and the expected wages that we calculate with our model are going to be smaller than these.

## Section 5

5 - What conclusions can be drawn from this study and the model implemented? Is the performance of the model correct?

There are several conclusions.

- The first and most obvious is that a linear regression model does not seem powerful enough for our problem. Our system seems too complex or too nuanced to be "summarized" only in a trend line (Let's not forget that no matter how many regularizations we do, they are polynomials of degree one, i.e. straight lines).

- The second is that the model that we choose seems that it is not going to have much importance because it throws very big errors and little reliability as it shows its $R^{2}$, to the point that, developing the exercise with or without *One Hot Encoder* I obtain similar results.

- Another important conclusion is that what is evident in many cases or may seem intuitive at first sight is not, because although the central limit theorem is fulfilled in this case, when applying non-parametric tests for the sample we have it tells us that it does not follow a normal distribution.

Considerations I would make if I wanted to go deeper:

- Look for polynomials with which to approximate the model more efficiently, such as degree 2 or 3rd degree polynomials (*cubic splines*).

- Take more samples of the teachers to see if they can be approximated to a normal distribution or if there really is a bias to the right (median to the left of the mean).
